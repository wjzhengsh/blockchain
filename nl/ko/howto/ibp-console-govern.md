---

copyright:
  years: 2019
lastupdated: "2019-05-16"

keywords: network components, IBM Cloud Kubernetes Service, allocate resources, batch timeout, channel update, reallocate resources

subcollection: blockchain

---

{:new_window: target="_blank"}
{:shortdesc: .shortdesc}
{:screen: .screen}
{:codeblock: .codeblock}
{:note: .note}
{:important: .important}
{:tip: .tip}
{:pre: .pre}

# 컴포넌트 관리
{: #ibp-console-govern}

CA, 피어, 순서 지정자, 조직 및 채널을 작성한 후 콘솔을 사용하여 이 컴포넌트를 업데이트할 수 있습니다.
{:shortdesc}

{{site.data.keyword.blockchainfull_notm}} Platform 베타 평가판을 사용 중인 경우 콘솔의 일부 패널이 GA(Generally Available) 서비스 인스턴스에 따라 최신 상태로 유지되는 현재 문서와 일치하지 않을 수 있습니다. 모든 최신 기능의 이점을 얻으려면 지금 [{{site.data.keyword.blockchainfull_notm}} Platform on {{site.data.keyword.cloud_notm}} 시작하기](/docs/services/blockchain/howto/ibp-v2-deploy-iks.html#ibp-v2-deploy-iks)의 지시사항에 따라 새 GA 서비스 인스턴스를 프로비저닝하는 것이 좋습니다. 

**대상 독자:** 이 주제는 블록체인 네트워크를 작성, 모니터링 및 관리할 책임이 있는 네트워크 운영자를 위해 설계되었습니다.

## {{site.data.keyword.cloud_notm}} Kubernetes Service가 콘솔과 상호작용하는 방법
{: #ibp-console-govern-iks-console-interaction}

네트워크 운영자는 CPU, 메모리 및 스토리지 사용량을 모니터해야 하고 노드의 작성 또는 크기 변경을 시도하기 **전에** 충분한 리소스가 사용 가능한지 확인해야 합니다.
{:important}

{{site.data.keyword.blockchainfull_notm}} Platform 콘솔의 인스턴스 및 {{site.data.keyword.cloud_notm}} Kubernetes Service 클러스터가 클러스터에서 사용 가능한 리소스에 대해 직접 통신하지 않으므로, 콘솔을 사용하여 컴포넌트를 배치하거나 컴포넌트의 크기를 조정하는 프로세스는 다음 패턴을 따라야 합니다.

1. **배치할 크기를 지정하십시오**. 콘솔에서 CA, 피어 및 순서 지정자를 위한 **리소스 할당** 패널은 각 노드마다 기본 CPU, 메모리 및 스토리지 할당을 제공합니다. 유스 케이스에 따라 이 값을 조정해야 할 수도 있습니다. 확실하지 않은 경우 기본 할당부터 시작하고 필요한 경우 기본 할당을 조정하십시오. 이와 마찬가지로 **리소스 할당** 패널은 기존 리소스 할당을 표시합니다.

  클러스터에 필요한 스토리지 및 컴퓨팅의 양은 이 목록 다음에 있는 차트를 참조하십시오. 여기에는 피어, 순서 지정자 및 CA에 대한 현재 기본값이 포함되어 있습니다.

2. **{{site.data.keyword.cloud_notm}} Kubernetes Service 클러스터에 충분한 리소스가 있는지 확인하십시오**. Kubernetes 리소스를 모니터하려면 {{site.data.keyword.cloud_notm}} Kubernetes 대시보드와 결합하여 [{{site.data.keyword.cloud_notm}} SysDig ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](https://www.ibm.com/cloud/sysdig "IBM Cloud Monitoring with Sysdig") 도구를 사용하는 것이 좋습니다. 리소스를 배치하거나 리소스의 크기를 조정하는 데 충분한 공간이 부족한 경우 {{site.data.keyword.cloud_notm}} Kubernetes Service 클러스터의 크기를 늘려야 합니다. 클러스터의 크기를 늘리는 방법에 대한 자세한 정보는 [클러스터 스케일링 ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](/docs/containers?topic=containers-ca#ca "클러스터 스케일링")을 참조하십시오. 클러스터에 충분한 공간이 있으면 3단계로 계속 진행할 수 있습니다.
3. **콘솔을 사용하여 노드를 배치하거나 노드의 크기를 조정하십시오**. 팟(Pod)이 실행 중인 작업자 노드에 리소스가 부족한 경우 새 대형 작업자 노드를 클러스터에 추가한 후 기존 작업 노드를 삭제할 수 있습니다.

| **컴포넌트**(모든 컨테이너) | CPU  | 메모리(GB) | 스토리지(GB) |
|--------------------------------|---------------|-----------------------|------------------------|
| **피어**                       | 1.1          | 2.2                   | 200(피어용 100GB 및 CouchDB용 100GB 포함) |
| **CA**                         | .1            | .2                    | 20                     |
|**순서 지정자**                    | .45           | .9                    | 100                    |

사용자가 노드를 완전히 중단하거나 삭제하지 않고 비용을 최소화하려는 경우 노드를 최소 0.001CPU(1milliCPU)까지 아래로 스케일링항 수 있습니다. 이 CPU 양을 사용하는 경우에는 이 노드가 작동하지 않습니다.

이 주제의 수치를 정확하게 표시하려고 노력하지만 클러스터에 충분한 공간이 있는 것으로 표시되는 경우에도 노드가 배치되지 않을 수 있다는 점에 유의하십시오. Kubernetes 대시보드를 참조하여 컴포넌트가 배치되는 시기와 배치되지 않은 경우 오류 메시지를 확인해야 합니다. 리소스 부족으로 인해 컴포넌트가 배치되지 않은 경우에는 클러스터에 충분한 공간이 있는 것으로 보이더라도 컴포넌트를 배치하려면 추가 클러스터 리소스를 배치해야 할 수 있습니다.
{:important}

## 리소스 할당
{: #ibp-console-govern-allocate-resources}

무료 클러스터의 사용자는 노드와 연관된 컨테이너에 대해 **기본 크기를 사용해야** 하지만, 유료 클러스터의 사용자는 노드 작성 중에 이 값을 설정할 수 있습니다.

콘솔의 **리소스 할당** 패널은 노드 작성과 연관된 여러 필드에 대한 기본값을 제공합니다. 이 값에는 시작하는 데 유용한 방법이 표시되어 있으므로 해당 값이 선택됩니다. 그러나 모든 유스 케이스는 다릅니다. 이 주제에서는 이 값을 찾기 위한 방법을 설명하지만, 궁극적으로 사용자는 노드를 모니터하고 노드에 적합한 크기를 찾아야 합니다. 그러므로 사용자가 기본값과는 다른 값이 필요하다고 확신하는 경우를 제외하고는 실현 가능한 측면에서 이 기본값을 사용하고 나중에 이 값을 조정하는 것이 좋습니다. {{site.data.keyword.blockchainfull_notm}} Platform을 기반으로 한 Hyperledger Fabric의 성능 및 스케일에 대한 개요는 [Answering your questions on Hyperledger Fabric performance and scale ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](https://www.ibm.com/blogs/blockchain/2019/01/answering-your-questions-on-hyperledger-fabric-performance-and-scale/ "Hyperledger Fabric 성능 및 스케일 관련 블로그")을 참조하십시오.

노드와 연관된 모든 컨테이너에 **CPU** 및 **메모리**가 있으며, 피어, 순서 지정자 및 CA와 연관된 특정 컨테이너에도 **스토리지**가 있습니다. {{site.data.keyword.cloud_notm}}에서 사용할 수 있는 여러 스토리지 옵션에 대한 자세한 정보는 [스토리지 옵션 ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](/docs/containers?topic=containers-kube_concepts#kube_concepts "Kubernetes 스토리지 옵션")을 참조하십시오. 사용할 스토리지 클래스를 결정하기 전에 옵션을 충분히 검토하십시오.

콘솔 및 {{site.data.keyword.cloud_notm}} Kubernetes Service 대시보드를 사용하여 CPU와 메모리를 변경할 수 있으나, 노드를 작성한 후에는 {{site.data.keyword.cloud_notm}} CLI를 사용해야만 스토리지를 나중에 변경할 수 있습니다.
{:note}

모든 노드에는 콘솔과 노드 간 통신 계층을 부트스트랩하는 gRPC 웹 프록시 컨테이너가 있습니다. 이 컨테이너는 고정된 리소스 값을 보유하고 있으며 리소스 할당 패널에 포함되어 있음에 따라 노드를 배치하기 위해 Kubernetes 클러스터에 필요한 공간의 양을 정확하게 예측하여 해당 값을 제공합니다. 이 컨테이너의 값을 변경할 수 없으므로 다음 절에서는 gRPC 웹 프록시를 다루지 않습니다.

### 인증 기관(CA)
{: #ibp-console-govern-CA}

트랜잭션 프로세스와 크게 연관된 피어 및 순서 지정자와는 달리 CA는 ID 등록 및 MSP 작성에만 연관됩니다. 이는 더 적은 양의 CPU와 메모리가 필요함을 의미합니다. CA에 중점을 두기 위해 사용자는 요청을 모두 처리해야 하거나(API 및 스크립트를 사용할 가능성이 있음) 너무 많은 인증서를 발행하여 스토리지가 부족한 상태입니다. 일반적인 오퍼레이션에서는 두 사항 모두 발생하면 안 되며, 항상 이들 값은 특정 유스 케이스의 요구사항을 반영해야 합니다.

CA에는 조정할 수 있는 하나의 연관된 컨테이너만 있습니다.

* **CA 자체**: CA가 발행하는 모든 인증서의 사본을 저장할 뿐만 아니라 내부 CA 프로세스(예: 노드 및 사용자 등록)를 캡슐화합니다.

#### 작성 중 CA 크기 지정
{: #ibp-console-govern-CA-sizing-creation}

gRPC 웹 프록시 서버의 값을 변경할 수 없으므로 CA에는 유의해야 하는 값이 포함된 하나의 컨테이너만 있습니다.

| 리소스 | 증가 조건 |
|-----------------|-----------------------|
| **CA 컨테이너 CPU 및 메모리** | CA에 등록이 쇄도할 것으로 예상되는 경우 |
| **CA 스토리지** | 이 CA를 사용하여 많은 수의 사용자 및 애플리케이션을 등록할 계획인 경우 |

### 피어
{: #ibp-console-govern-peers}

피어에는 조정할 수 있는 세 가지 연관된 컨테이너가 있습니다.

- **피어 자체**: 속하는 모든 채널에 대해 내부 피어 프로세스(예: 트랜잭션 유효성 검증) 및 블록체인(즉, 트랜잭션 히스토리)을 캡슐화합니다. 피어의 스토리지에는 피어에 설치된 스마트 계약도 포함됩니다.
- **CouchDB**: 여기서 피어의 상태 데이터베이스가 저장됩니다. 각 채널에 별도의 상태 데이터베이스가 있음을 상기하십시오.
- **스마트 계약**: 트랜잭션 중에 관련 스마트 계약이 "호출됨"(즉, 실행)을 상기하십시오. 피어에 설치하는 모든 스마트 계약은 Docker-in-Docker 컨테이너로 알려진 스마트 계약 컨테이너 내부에 있는 별도의 컨테이너에서 실행됩니다.

#### 작성 중 피어 크기 지정
{: #ibp-console-govern-peers-sizing-creation}

[ {{site.data.keyword.cloud_notm}} Kubernetes Service가 콘솔과 상호작용 하는 방법](/docs/services/blockchain/howto/ibp-console-govern.html#ibp-console-govern-iks-console-interaction) 절에서 설명한 대로 이러한 피어 컨테이너에 기본값을 사용하고 나중에 활용 방식이 명확해질 때 조정하는 것이 좋습니다. 

| 리소스 | 증가 조건 |
|-----------------|-----------------------|
| **피어 컨테이너 CPU 및 메모리** |높은 트랜잭션 처리량을 즉시 예측하는 경우 |
| **피어 스토리지** | 이 피어에 다수의 스마트 계약을 설치하고 여러 채널에 참여할 것을 예측하는 경우. 이 스토리지는 피어가 참여할 모든 채널에서 스마트 계약을 저장하는 데에도 사용됨을 상기하십시오. 10,000바이트(10k) 범위의 "소형" 트랜잭션을 예측한다는 점에 유의하십시오. 기본 스토리지가 100G이면 이는 총 트랜잭션을 최대 1000만으로 늘려야 피어 스토리지에 적합함을 의미합니다(실제로, 트랜잭션이 크기에 따라 달라질 수 있고 해당 수에 스마트 계약이 포함되지 않음에 따라 최대 수는 이 값보다 미만임). 따라서 100G은 필요한 스토리지보다 훨씬 더 많아 보이지만, 스토리지는 상대적으로 비싸지 않으며 스토리지를 늘리는 프로세스가 CPU 또는 메모리를 늘리는 프로세스보다 훨씬 더 어렵다는 점을 기억하십시오. |
| **CouchDB 컨테이너 CPU 및 메모리** | 대형 상태 데이터베이스에 맞게 조회의 볼륨이 높아질 것을 예측하는 경우. [인덱스 ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](https://hyperledger-fabric.readthedocs.io/en/release-1.4/couchdb_as_state_database.html#couchdb-indexes "Hyperledger Fabric 인덱스 문서")를 사용하면 이 현상이 다소 완화될 수 있습니다. 그렇지만 볼륨이 높으면 CouchDB가 한계에 도달할 수 있으며, 이에 따라 조회 및 트랜잭션 제한시간이 초과될 수 있습니다. |
| **CouchDB(원장 데이터) 스토리지** | 많은 채널에서 처리량이 높아질 것을 예측하며 인덱스를 사용할 계획이 없는 경우. 그러나 피어 스토리지와 마찬가지로 기본 CouchDB 스토리지는 100G이며, 이는 상당한 양입니다. |
| **스마트 계약 컨테이너 CPU 및 메모리** | 채널에서 처리량이 높아질 것을 예측하는 경우(특히 다중 스마트 계약이 즉시 즉시 호출될 경우)|

### 노드 순서 지정
{: #ibp-console-govern-ordering-nodes}

순서 지정자가 상태 DB를 유지보수하지 않고 스마트 계약을 호스팅하지 않으므로 피어보다 더 적은 컨테이너가 필요합니다. 그러나 블록체인이 채널 구성을 저장하는 위치임에 따라 순서 지정자는 블록체인(트랜잭션 히스토리)을 호스팅하지 않고, 순서 지정자는 해당 역할을 수행하기 위해 최신 채널 구성을 알고 있어야 합니다.

CA와 마찬가지로 순서 지정자에는 조정할 수 있는 하나의 연관된 컨테이너만 있습니다.

* **순서 지정자 자체**: 호스팅하는 모든 채널에 대해 내부 순서 지정자 프로세스(예: 트랜잭션 유효성 검증) 및 블록체인을 캡슐화합니다.

#### 작성 중 순서 지정자 크기 지정
{: #ibp-console-govern-orderer-sizing-creation}

[{{site.data.keyword.cloud_notm}} Kubernetes Service가 콘솔과 상호작용 하는 방법](/docs/services/blockchain/howto/ibp-console-govern.html#ibp-console-govern-iks-console-interaction) 절에서 설명한 대로 이러한 순서 지정자 컨테이너에 기본값을 사용하고 나중에 활용 방식이 명확해질 때 조정하는 것이 좋습니다. 

| 리소스 | 증가 조건 |
|-----------------|-----------------------|
| **순서 지정자 컨테이너 CPU 및 메모리** |높은 트랜잭션 처리량을 즉시 예측하는 경우 |
| **순서 지정자 스토리지** | 이 순서 지정자가 많은 채널에서 순서 지정 서비스의 일부가 될 것을 예측하는 경우. 순서 지정자가 일부가 되는 모든 채널에 대한 블록체인의 사본을 보관하고 있음을 상기하십시오. 순서 지정자의 기본 스토리지는 피어 자체의 컨테이너와 동일한 100G입니다. |

순서 지정자 노드의 CPU 및 메모리를 피어 크기의 두 배로 만드는 것은 필수는 아니지만 우수 사례로 간주됩니다. 순서 지정 서비스가 과도하게 설정되면 제한시간이 초과되고 트랜잭션 삭제가 시작되며, 이에 따라 트랜잭션을 다시 제출해야 합니다. 이는 지속적인 운영을 위해 노력하는 단일 피어보다 네트워크에 더욱 나쁜 영향을 줄 수 있습니다. Raft 순서 지정 서비스 구성에서 과도하게 설정된 선행 노드로 하트비트 메시지 전송, 선행 선출 트리거 및 트랜잭션 순서 지정의 임시 중단이 중지될 수 있습니다. 이와 유사하게, 팔로워 노드는 메시지를 누락시킬 수 있고 아무 것도 필요하지 않은 선행 선출을 트리거하려고 할 수 있습니다.
{:important}

## 리소스 재할당
{: #ibp-console-govern-reallocate-resources}

노드의 크기를 조정한 후에는 컨테이너가 다시 빌드되기 때문에 크기 변경이 적용되기 전에 지연이 발생할 수 있습니다.
{:important}

위에서 언급한 대로, Kubernetes 리소스 사용량을 모니터하려면 {{site.data.keyword.cloud_notm}} Kubernetes 대시보드를 결합하여 [{{site.data.keyword.cloud_notm}} SysDig ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](https://www.ibm.com/cloud/sysdig "{{site.data.keyword.cloud_notm}} Monitoring with Sysdig") 도구를 사용하는 것이 좋습니다. 작업자 노드가 리소스 외부에서 실행되는 경우 새 대형 작업자 노드를 클러스터에 추가한 후 기존 작업 노드를 삭제할 수 있습니다.
{:note}

{{site.data.keyword.cloud_notm}} Kubernetes Service에 충분한 리소스를 배치하는 것이 더 쉽고 먼저 {{site.data.keyword.cloud_notm}} Kubernetes Service에 대한 배치를 늘리지 않고도 원하는 대로 팟(Pod) 및 작업자 노드를 확장할 수 있지만, {{site.data.keyword.cloud_notm}} Kubernetes Service의 배치가 늘어나면 비용도 증가합니다. 사용자는 이 옵션을 신중하게 고려하고 사용자가 선택하는 옵션에 관계 없이 절충안을 알고 있어야 합니다.

노드와 연관되는 컨테이너에 지정하는 리소스를 재할당하기 위해 다음 방법 중 하나를 사용할 수 있습니다.

1. **{{site.data.keyword.cloud_notm}} Kubernetes Service Autoscaler**를 사용하십시오. Autoscaler는 팟(Pod) 스펙 설정 및 리소스 요청에 대한 응답으로 작업자 노드를 위 또는 아래로 스케일링합니다. {{site.data.keyword.cloud_notm}} Kubernetes Service Autoscaler 및 설정 방법에 대한 자세한 정보는 {{site.data.keyword.cloud_notm}} 문서의 {{site.data.keyword.cloud_notm}}[클러스터 스케일링 ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](/docs/containers?topic=containers-ca#ca "클러스터 스케일링")을 참조하십시오. 리소스를 조정하기 위해 Autoscaler를 사용하면 {{site.data.keyword.cloud_notm}} Kubernetes Service 계정에 대한 비용이 부과되며, 이는 사용량에 따라 달라집니다.
2. **수동으로 스케일링하십시오**. 여기에는 콘솔 및 {{site.data.keyword.cloud_notm}} Kubernetes Service 클러스터의 사용량 모니터링이 포함됩니다. Autoscaler 사용보다 더 많은 수동 단계가 포함되지만, 사용자가 {{site.data.keyword.cloud_notm}} Kubernetes Service 계정에 부과되는 요금에 대해서는 언제나 확실히 알고 있다는 장점이 있습니다.

수동으로 스케일링하려면 **노드** 페이지에서 조정할 노드를 클릭한 후 **사용량** 탭을 클릭하십시오. **재할당**이라는 단추가 표시되며, 이를 통해 노드 작성 시 표시된 탭과 매우 유사한 **리소스 재할당** 탭이 실행됩니다. 사용 가능한 리소스의 양을 줄이려면 단순히 더 적은 값만 제공하고 탭의 **리소스 재할당**을 클릭한 후 **요약** 페이지를 클릭하십시오.

노드에 맞게 CPU와 메모리의 양을 늘리려면 **리소스 할당** 탭을 사용하여 값을 늘리십시오. 페이지 맨 아래에 있는 흰색 상자에서 새 값이 추가됩니다. **리소스 재할당**을 클릭하면 **요약** 페이지에서 이 값이 **VPC** 양으로 변환되며 이는 청구서를 계산하는 데 사용됩니다. 그런 다음 {{site.data.keyword.cloud_notm}} Kubernetes Service 대시보드로 이동하여 클러스터에 이 재할당을 처리하는 데 충분한 리소스가 있는지 확인해야 합니다. 리소스가 충분하면 **리소스 재할당**을 클릭할 수 있습니다. 리소스가 충분하지 않으면 {{site.data.keyword.cloud_notm}} Kubernetes Service 대시보드를 사용하여 클러스터의 크기를 늘려야 합니다.

스토리지를 늘리는 데 사용할 메소드는 클러스터에 대해 선택한 스토리지 클래스에 따라 달라집니다. 스토리지를 늘리는 방법에 대한 정보는 [스토리지 옵션 ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](/docs/containers?topic=containers-kube_concepts#kube_concepts "스토리지 옵션") 문서를 참조하십시오. 피어 또는 순서 지정자에서 스토리지를 다 써버리는 경우 대형 파일 시스템에서 새 피어 또는 순서 지정자를 배치해야 하고 동일한 채널의 기타 컴포넌트를 통해 동기화할 수 있도록 해야 합니다.

콘솔을 사용하여 늘릴 수 있는 CPU 및 메모리와는 달리({{site.data.keyword.cloud_notm}} Kubernetes Service 클러스터에 사용 가능한 리소스가 있는 경우) {{site.data.keyword.cloud_notm}} CLI를 사용하여 노드의 스토리지를 늘려야 합니다. 이를 수행하는 방법에 대한 튜토리얼은 [기존 스토리지 디바이스의 크기 및 IOPS 변경 ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](/docs/containers?topic=containers-file_storage#file_change_storage_configuration "기존 스토리지 디바이스의 크기 및 IOPS 변경")을 참조하십시오.

## 채널 구성 업데이트
{: #ibp-console-govern-update-channel}

채널 작성과 채널 업데이트의 목표는 동일하지만 사용자에게 채널이 구성이 해당 유스 케이스에 가능한 잘 맞는지 확인하는 기능을 제공하므로 사실상 두 개의 프로세스는 콘솔에서 매우 다른 **as 태스크**입니다. [채널 작성](/docs/services/blockchain/howto/ibp-console-build-network.html#ibp-console-build-network-create-channel)에 대한 문서에서 이는 **단일 조직**에서 수행되는 프로세스임을 상기하십시오. 조직이 채널의 순서 지정 서비스가 될 순서 지정 서비스의 컨소시엄 구성원이면 원하는 방식으로 채널을 작성할 수 있습니다. 채널에 임의의 이름 지정, 조직 추가(해당 조직이 컨소시엄의 구성원인 경우에 한해), 해당 조직에 권한 지정, 액세스 제어 목록 설정 등의 작업을 수행할 수 있습니다.

다른 조직은 이 채널에 참여할지 여부(예: 피어를 이 채널에 가입시킬지 여부)를 선택할 수 있지만 채널 매개변수를 선택하는 협업 프로세스는 조직이 콘솔을 사용하여 채널을 작성하기 전에 대역 외에서 발생하는 것으로 가정합니다.

채널 업데이트는 다릅니다. **콘솔 내**에서 발생하며 {{site.data.keyword.blockchainfull_notm}} Platform이 작동하는 방식의 기초가 되는 협업 통제 프로시저를 따릅니다. 이 협업 프로세스에는 채널의 관리자 역할이 있는 조직에 채널 구성 업데이트 요청을 전송하는 작업이 포함됩니다. 이러한 조직을 채널 **운영자(operators)**라고도 합니다.

채널을 업데이트하려면 **채널** 탭 내부의 채널을 클릭하십시오. 페이지의 맨 위에 있는 채널 이름 옆의 **설정** 단추를 클릭하십시오. 채널을 작성하는 데 사용하는 패널과 매우 유사한 패널이 표시됩니다.

### 업데이트할 수 있는 채널 구성 매개변수
{: #ibp-console-govern-update-channel-available-parameters}

채널이 작성된 후 채널의 구성 매개변수 중 일부(전체는 아님)를 변경할 수 있습니다. 또한 채널 작성 중에 업데이트할 수 있고 사용할 수 없는 매개변수는 하나뿐입니다.

**채널 이름**이 회색으로 표시되고 편집할 수 없음을 알 수 있습니다. 이는 채널 이름이 작성된 후 변경될 수 없다는 사실을 나타냅니다. 또한 채널이 작성된 후 채널의 순서 지정 서비스도 변경할 수 없으므로 순서 지정 서비스 표시 이름이 없음을 확인하십시오.

그러나 다음 채널 구성 매개변수를 변경할 수 있습니다.

* **조직**. 패널의 이 섹션은 조직이 채널에 추가되거나 제거되는 방법입니다. 추가될 수 있는 조직은 드롭 다운 목록에 표시될 수 있습니다. 조직이 채널에 추가되려면 순서 지정 서비스 컨소시엄의 구성원이어야 합니다. 조직을 컨소시엄에 추가하는 방법에 대한 자세한 정보는 [트랜잭션을 수행할 수 있는 조직의 목록에 조직 추가](/docs/services/blockchain/howto/ibp-console-build-network.html#ibp-console-build-network-add-org)를 참조하십시오.

* **채널 업데이트 정책**. 채널의 업데이트 정책은 채널 구성의 업데이트를 승인해야 하는 조직의 수(채널에 있는 총 조직 수 중에서)를 지정합니다. 협업 관리와 효율적인 채널 구성 업데이트 처리 간의 적절한 균형을 유지하려면 이 설정을 다수의 도메인으로 설정하십시오. 예를 들어, 채널에 5개의 관리자가 있는 경우 `3 out of 5`를 선택하십시오.

* **블록 잘라내기 매개변수**. (고급 옵션) 기본 블록 잘라내기 매개변수에 대한 변경사항은 순서 지정 서비스 조직의 관리자가 서명해야 하므로 이러한 필드는 채널 작성 패널에 없습니다. 그러나 이 채널 구성은 채널에 있는 모든 관련 조직에 전송되기 때문에 블록 잘라내기 매개변수에 대한 변경사항이 포함된 채널 구성 업데이트 요청을 전송할 수 있습니다. 이러한 필드에 따라 순서 지정 서비스가 새 블록을 잘라내는 조건이 결정됩니다. 블록을 잘라낼 때 이러한 필드가 미치는 영향에 대한 정보는 [블록 잘라내기 매개변수](/docs/services/blockchain/howto/ibp-console-govern.html#ibp-console-govern-orderer-tuning-batch-size)를 참조하십시오.

* **액세스 제어 목록**. (고급 옵션) 리소스에 대한 세부 단위의 제어를 지정하려면 리소스에 대한 액세스를 조직과 해당 조직 내의 역할로 제한할 수 있습니다. 예를 들어, `ChaincodeExists` 리소스에 대한 액세스를 `Application/Admins`로 설정하는 것은 애플리케이션의 관리자만 `ChaincodeExists` 리소스에 액세스할 수 있음을 의미합니다.

리소스에 대한 액세스를 특정 조직으로 제한하는 경우 해당 조직만 리소스에 액세스할 수 있음에 유의하십시오. 다른 조직이 리소스에 액세스할 수 있도록 하려면 아래 필드를 사용하여 해당 조직을 하나씩 추가해야 합니다. 따라서 액세스 제어 의사결정을 신중하게 고려하십시오. 특정 방법으로 특정 리소스에 대한 액세스를 제한하면 채널이 작동하는 방식에 매우 부정적인 영향을 미칠 수 있습니다.
{:important}

콘솔은 단일 사용자에게 여러 조직을 소유하고 제어할 수 있는 기능을 제공하기 때문에 **채널 업데이트 수행자 조직** 섹션에서 채널 업데이트에 서명할 때 사용 중인 조직을 지정해야 합니다. 이 채널에서 둘 이상의 조직을 소유하는 경우 채널에서 소유하는 조직 중에서 서명할 조직을 선택할 수 있습니다. 선택한 **채널 업데이트 정책**에 따라 사용자가 소유한 다른 조직 중 하나 이상으로 요청에 서명하도록 요구하는 알림을 받을 수 있습니다.

**블록 잘라내기 매개변수** 중 하나를 변경하려고 시도하는 중이며 이 채널의 순서 지정 서비스 조직을 소유하는 경우 순서 지정 서비스 조직에 대한 필드가 표시됩니다. 드롭 다운 목록에서 관련 순서 지정 서비스 조직의 MSP를 선택하십시오. 순서 지정 서비스 조직의 관리자가 아닌 경우 블록 잘라내기 매개변수 중 하나를 변경하라는 요청을 작성할 수 있지만 요청이 전송되고 순서 지정 서비스 관리자가 요청에 서명해야 합니다.

### 서명 콜렉션 플로우
{: #ibp-console-govern-update-channel-signature-collection}

서명이 확인되려면 채널의 조직이 해당 조직을 나타내는 MSP(JSON 형식)를 채널의 다른 조직으로 내보내고 다른 조직의 MSP를 가져와야 합니다. MSP를 내보내려면 **조직** 화면에서 해당 MSP의 다운로드 단추를 클릭한 후 대역 외 다른 조직으로 전송하십시오. MSP의 JSON을 수신하면 **조직** 화면을 사용하여 가져오십시오.
{: important}

채널 구성 업데이트가 요청되면 서명할 권한이 있는 채널의 조직에 해당 요청이 전송됩니다. 예를 들어, 채널에 5개의 운영자(채널 관리자)가 있는 경우 5개 모두에 전송됩니다. 채널 구성 업데이트가 승인되려면 **채널 업데이트 정책**에 나열된 채널 운영자 수가 충족되어야 합니다. 이 정책이 `3 out of 5`를 나타내는 경우 채널 구성 업데이트가 5개의 운영자 모두에게 전송되고 이 중 3개가 요청에 서명하면 새 채널 구성 업데이트가 적용됩니다.

서명할 업데이트가 있는지 파악하고 업데이트에 서명하는 이 프로세스는 콘솔의 오른쪽 상단에 있는 **알림** 단추(종 모양)를 통해 처리됩니다. **알림** 단추에 파란색 점이 표시되면 평가할 보류 중인 요청이 있거나 채널 업데이트 이벤트에 대한 알림을 받았음을 의미합니다.

**알림** 단추를 클릭하면 수행할 수 있는 하나 이상의 조치가 있을 수 있습니다. 채널 구성 업데이트가 요청된 경우 `Review and update channel configuration`을 클릭하고 제안 중이거나 완료된(새 채널 구성이 승인된 경우) 채널 구성 업데이트에 대한 변경사항을 볼 수 있는 기능이 있습니다. 채널의 운영자이고 채널 구성 업데이트 요청을 승인하기에 충분한 서명이 수집되지 않은 경우 업데이트 요청에 서명할 수 있습니다.

채널 구성 업데이트에 반드시 서명해야 하는 것은 아니지만 채널 업데이트에 **반대하여** 서명할 수 있는 방법은 없습니다. 채널 구성 업데이트를 승인하지 않는 경우 단순히 패널을 닫고 대역 외 다른 채널 운영자에게 연락하여 의견을 표명할 수 있습니다. 그러나 채널 업데이트 정책을 충족하기에 충분한 수의 채널 운영자가 업데이트를 승인하면 새 구성이 적용됩니다.
{:note}

## 순서 지정자 조정
{: #ibp-console-govern-orderer-tuning}

블록체인 플랫폼의 성능은 트랜잭션 크기, 블록 크기, 네트워크 크기 및 하드웨어 한계와 같이 여러 변수의 영향을 받을 수 있습니다. 순서 지정자 노드에는 순서 지정자 처리량 및 성능을 제어하는 데 함께 사용될 수 있는 조정 매개변수 세트가 포함됩니다.  이 매개변수를 사용하면 순서 지정자가 트랜잭션을 처리하는 방식을 사용자 정의할 수 있으며, 이는 빈도 수는 높지만 소형 트랜잭션이 있는지 아니면 빈도 수는 적지만 대형 트랜잭션(더 적은 횟수로 도달함)에 따라 달라집니다. 기본적으로 트랜잭션 크기, 양 및 도달률을 기반으로 블록을 줄이는 시기를 결정할 수 있는 제어 권한이 제공됩니다.

다음 매개변수는 **노드** 탭에서 순서 지정자 노드를 클릭한 후 **설정** 아이콘을 클릭하여 콘솔에서 사용할 수 있습니다. **고급** 단추를 클릭하여 순서 지정자를 위한 **고급 채널 구성**을 여십시오.

### 블록 잘라내기 매개변수
{: #ibp-console-govern-orderer-tuning-batch-size}

블록을 줄이는 시기를 제어하려면 블록의 최대 트랜잭션 수 설정과 블록 크기 자체의 결합을 기반으로 하여 다음 세 가지 매개변수가 함께 적용됩니다.

- **절대 최대 바이트**  
  이 값을 순서 지정자가 줄일 수 있는 최대 블록 크기(바이트)로 설정합니다.  트랜잭션은 `Absolute max bytes` 값보다 클 수 없습니다. 일반적으로 이 설정은 안전하게 `Preferred max bytes`보다 2 - 10배 더 클 수 있습니다.    
  **참고**: 허용되는 최대 크기는 99MB입니다.
- **최대 메시지 수**   
  이 값을 단일 불록에 포함할 수 있는 최대 트랜잭션 수로 설정합니다.
- **원하는 최대 바이트**  
  이 값을 `Absolute max bytes` 미만의 이상적인 블록 크기(바이트)로 설정합니다. 보증이 포함되지 않은 최소 트랜잭션 크기는 약 1KB입니다.  필요한 보증마다 1KB를 추가하면 일반적인 트랜잭션 크기는 대략 3 - 4KB가 됩니다. 그러므로 `Preferred max bytes`의 값을 약 `Max message count * expected averaged tx size`로 설정하는 것이 좋습니다. 런타임 시 가능한 한 블록은 이 크기를 초과하지 않습니다. 트랜잭션이 블록으로 인해 이 크기가 초과하는 지점에 도달한 경우 블록은 줄어들고 해당 트랜잭션에 맞게 새 블록이 작성됩니다. 트랜잭션이 `Absolute max bytes`를 초과하지 않고 이 값을 초과하는 지점에 도달하는 경우 트랜잭션이 포함됩니다. 블록이 `Preferred max bytes`보다 큰 지점에 도달하는 경우 단일 트랜잭션만 포함되고 해당 트랜잭션 크기가 `Absolute max bytes`보다 클 수 없습니다.

이 매개변수는 순서 지정자의 처리량을 최적화하도록 구성될 수 있습니다.

### 일괄처리 제한시간 초과
{: #ibp-console-govern-orderer-tuning-batch-timeout}

**제한시간 초과** 값을 첫 번째 트랜잭션이 블록을 줄이기 전에 도달한 후 기다리는 시간(초)으로 설정하십시오. 이 값을 너무 짧게 설정하면 일괄처리를 원하는 크기로 채울 수 없게 됩니다. 이 값을 너무 길게 설정하면 순서 지정자가 블록 및 전체 성능이 저하될 때까지 기다릴 수 있습니다. 일반적으로 최소 `max message count / maximum transactions per second`로 `Batch timeout` 값을 설정하는 것이 좋습니다.

이 매개변수를 수정하는 경우 순서 지정자의 기존 채널 작동에는 영향을 주지 않습니다. 대신, 순서 지정자 구성의 변경사항은 이 순서 지정자에서 작성하는 새 채널에만 적용됩니다.
{:important}

## 채널 수정
{: #ibp-console-govern-channels}

### 앵커 피어 구성
{: #ibp-console-govern-channels-anchor-peers}

서비스 발견 및 개인용 데이터가 작동하려면 교차 조직의 [gossip ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](https://hyperledger-fabric.readthedocs.io/en/release-1.4/gossip.html "gossip 데이터 분배 프로토콜")을 사용할 수 있어야 하므로 각 조직에 대해 앵커 피어가 존재해야 합니다. 이 앵커 피어는 특별한 **유형**의 피어가 아니지만 조직이 다른 조직에 알려져 있고 부트스트랩 교차 조직의 gossip을 수행하는 피어입니다. 그러므로 하나 이상의 [앵커 피어 ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](https://hyperledger-fabric.readthedocs.io/en/release-1.4/gossip.html#anchor-peers "앵커 피어")가 콜렉션의 각 조직마다 정의되어야 합니다.

피어를 앵커 피어가 되도록 구성하려면 **채널** 탭을 클릭하고 스마트 계약이 인스턴스화되는 채널을 여십시오.
 - **채널 세부사항** 탭을 클릭하십시오.
 - 아래로 스크롤하여 앵커 피어 테이블로 이동한 후 **앵커 피어 추가**를 클릭하십시오.
 - 조직에서 앵커 피어의 역할을 수행할 콜렉션 정의의 각 조직에서 하나 이상의 피어를 선택하십시오. 중복성의 이유로 콜렉션의 각 조직에서 둘 이상의 피어를 선택할 것을 고려할 수 있습니다.
